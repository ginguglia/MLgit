# instructions needed to support both python 2 and python 3 and import functions
from __future__ import division, print_function, unicode_literals

import numpy as np
import os
import tensorflow as tf
import time

# to make this notebook's output stable across runs with a specific seed
def reset_graph(seed=42):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)
#tf.set_random_seed(1234)

# instructions to plot figures
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "cnn"

def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)
    
# To start taking time
start = time.perf_counter()    
start_time= time.time()    
height = 28
width = 28
channels = 1
n_inputs = height * width

conv1_fmaps = 64
conv1_ksize = 5
conv1_stride = 1
conv1_pad = "SAME"

pool1_fmaps = conv1_fmaps


conv2_fmaps = 64
conv2_ksize = 3
conv2_stride = 1
conv2_pad = "VALID"


conv3_fmaps = 64
conv3_ksize = 5
conv3_stride = 1
conv3_pad = "SAME"

pool2_fmaps = conv2_fmaps

conv4_fmaps = 64
conv4_ksize = 1
conv4_stride = 1
conv4_pad = "SAME"

conv5_fmaps = 64
conv5_ksize = 3
conv5_stride = 1
conv5_pad = "SAME"

conv6_fmaps = 64
conv6_ksize = 5
conv6_stride = 1
conv6_pad = "SAME"

conv7_fmaps = 64
conv7_ksize = 1
conv7_stride = 1
conv7_pad = "VALID"

pool3_fmaps = conv7_fmaps

conv8_fmaps = 64
conv8_ksize = 3
conv8_stride = 1
conv8_pad = "SAME"

conv9_fmaps = 64
conv9_ksize = 5
conv9_stride = 1
conv9_pad = "SAME"

conv10_fmaps = 64
conv10_ksize = 1
conv10_stride = 1
conv10_pad = "SAME"
pool4_fmaps = conv10_fmaps


n_fc1 = 64
n_fc2 = 64
n_fc3 = 64

n_outputs = 10

reset_graph()

with tf.name_scope("inputs"):
    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name="X")
    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])
    y = tf.placeholder(tf.int32, shape=[None], name="y")

conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,
                         strides=conv1_stride, padding=conv1_pad,
                         activation=tf.nn.relu, name="conv1")

with tf.name_scope("pool1"):
    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding="VALID")
 #   pool1_flat = tf.reshape(pool1, shape=[-1, pool1_fmaps * 14 * 14])

    
conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,
                         strides=conv2_stride, padding=conv2_pad,
                         activation=tf.nn.relu, name="conv2")
conv3 = tf.layers.conv2d(conv2, filters=conv3_fmaps, kernel_size=conv3_ksize,
                         strides=conv3_stride, padding=conv3_pad,
                         activation=tf.nn.relu, name="conv3")

conv4 = tf.layers.conv2d(conv3, filters=conv4_fmaps, kernel_size=conv4_ksize,
                         strides=conv4_stride, padding=conv4_pad,
                         activation=tf.nn.relu, name="conv4")
with tf.name_scope("pool2"):
    pool2 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="VALID")
#    pool2_flat = tf.reshape(pool2, shape=[-1, pool2_fmaps * 7 * 7])

conv5 = tf.layers.conv2d(pool2, filters=conv5_fmaps, kernel_size=conv5_ksize,
                         strides=conv5_stride, padding=conv5_pad,
                         activation=tf.nn.relu, name="conv5")

conv6 = tf.layers.conv2d(conv5, filters=conv6_fmaps, kernel_size=conv6_ksize,
                         strides=conv6_stride, padding=conv6_pad,
                         activation=tf.nn.relu, name="conv6")

conv7 = tf.layers.conv2d(conv6, filters=conv7_fmaps, kernel_size=conv7_ksize,
                         strides=conv7_stride, padding=conv7_pad,
                         activation=tf.nn.relu, name="conv7")
with tf.name_scope("pool3"):
    pool3 = tf.nn.max_pool(conv7, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")
 #   pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 3 * 3])
    
conv8 = tf.layers.conv2d(pool3, filters=conv8_fmaps, kernel_size=conv8_ksize,
                         strides=conv8_stride, padding=conv8_pad,
                         activation=tf.nn.relu, name="conv8")
conv9 = tf.layers.conv2d(conv8, filters=conv9_fmaps, kernel_size=conv9_ksize,
                         strides=conv9_stride, padding=conv9_pad,
                         activation=tf.nn.relu, name="conv9")
conv10 = tf.layers.conv2d(conv9, filters=conv10_fmaps, kernel_size=conv10_ksize,
                         strides=conv10_stride, padding=conv10_pad,
                         activation=tf.nn.relu, name="conv10")
with tf.name_scope("pool4"):
    pool4 = tf.nn.max_pool(conv10, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")
    pool4_flat = tf.reshape(pool4, shape=[-1, pool4_fmaps * 2 * 2])
    
with tf.name_scope("fc1"):
    fc1 = tf.layers.dense(pool4_flat, n_fc1, activation=tf.nn.relu, name="fc1")
    
with tf.name_scope("fc2"):
    fc2 = tf.layers.dense(fc1, n_fc2, activation=tf.nn.relu, name="fc2")

with tf.name_scope("fc3"):
    fc3 = tf.layers.dense(fc2, n_fc3, activation=tf.nn.relu, name="fc3")
    
with tf.name_scope("output"):
    logits = tf.layers.dense(fc3, n_outputs, name="output")
    Y_proba = tf.nn.softmax(logits, name="Y_proba")

with tf.name_scope("train"):
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)
    loss = tf.reduce_mean(xentropy)
    optimizer = tf.train.AdamOptimizer()
    training_op = optimizer.minimize(loss)

with tf.name_scope("eval"):
    correct = tf.nn.in_top_k(logits, y, 1)
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))

with tf.name_scope("init_and_save"):
    init = tf.global_variables_initializer()
    saver = tf.train.Saver()
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/")
n_epochs = 100
batch_size = 100


training=[]
testing=[]
losstrain=[]
losstest=[]
with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        for iteration in range(mnist.train.num_examples // batch_size):
            X_batch, y_batch = mnist.train.next_batch(batch_size)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
        loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})
        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})
        loss_test = loss.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})
        training.append(acc_train)
        testing.append(acc_test)
        losstrain.append(loss_train)
        losstest.append(loss_test)
        print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)
        


        save_path = saver.save(sess, "./my_mnist_model")
# To evaluate total execution time
elapsed = time.perf_counter() - start
end_time = time.time()

# To plot accuracies and losses

plt.subplot(1,2,1)
plt.plot(training,label='training accuracy')
plt.plot(testing,label='testing accuracy')
plt.legend(loc='lower right');

plt.subplot(1,2,2)
plt.plot(losstrain,label='training loss')
plt.plot(losstest,label='testing loss')
plt.legend(loc='upper right');

# To print total execution time

print('Elapsed time with preformance counter %.3f seconds.' % elapsed)
print('Total processing time:',end_time - start_time)

# To show shapes

print("conv1",conv1.shape)
print("pool1",pool1.shape)
print("conv2",conv2.shape)
print("conv3",conv3.shape)
print("conv4",conv4.shape)
print("pool2",pool2.shape)
print("conv5",conv5.shape)
print("conv6",conv6.shape)
print("conv7",conv7.shape)
print("pool3",pool3.shape)
print("conv8",conv8.shape)
print("conv9",conv9.shape)
print("conv10",conv10.shape)
print("pool4",pool4.shape)
print("fc1",fc1.shape)
print("fc2",fc2.shape)
print("fc3",fc3.shape)
